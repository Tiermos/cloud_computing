 Lab 5 Docker Fundamentals Lab Report

1. Objective
The goal of this lab is to gain hands-on experience with Docker by exploring its core concepts—including containers, images, and Docker Compose—and practicing essential operations such as publishing ports, overriding default commands, persisting data, sharing local files, and running multi-container applications. Through reading official documentation and performing practical exercises, this report documents the learning process, key observations, and outcomes.

2. Core Concepts Summary
2.1 What is a Container?
A container is an isolated process that includes an application and all its dependencies, libraries, and configuration files. It runs on the host operating system’s kernel but is separated from other processes. This isolation prevents interference with or from other software on the host. Containers enable consistent application behavior across different environments without code changes.

2.2 What is an Image?
A container image is a file that contains all the code, binaries, libraries, and configuration needed to run an application in a container. Images are immutable—once created, they cannot be changed—and are built from multiple layers that represent file system changes. They serve as templates for creating containers and can be shared through registries like Docker Hub.

2.3 What is Docker Compose?
Docker Compose is a tool for defining and running multi-container applications using a single YAML file. This file specifies the services, networks, volumes, and configurations needed for the application. With one command (docker compose up), all containers are started and connected automatically. It simplifies managing, updating, and tearing down complex applications compared to running each container separately.

3. Hands-on Experiments
3.1 Publishing Ports
First, I create a new directory and create a new file named lab5_1.yaml within it.
The content defines the service image as docker/welcome-to-docker and maps the host's port 8080 to the container's port  80 through the ports field.   Then open the terminal in this directory and execute the
docker compose -f lab5_1.yaml up
command to start the service.  
 At this point,  we can view the corresponding container and its port mapping information on the Containers page of Docker Desktop.  The relevant configuration content and the final displayed welcome page are respectively shown in the lab5_1.yaml file and the screenshot Publishing Ports.png.

3.2 Overriding Container Defaults
I executed multiple docker run commands in PowerShell one by one: First, I started a PostgreSQL container, mapped port 5432 of the host and set a password; Then start the second container and map it to port 5433 of the host to avoid conflicts; Then create a custom network called mynetwork and start a third container to connect to this network, mapping to port 5434 of the host. Finally, run a PostgreSQL container with a limited memory of 512MB and a CPU of 0.5 cores. After the operation was completed, I confirmed in Docker Desktop that all containers were running normally, and the port mapping and network configuration were consistent with expectations.

3.3 Persisting Container Data
I run the PostgreSQL 18 container on Windows using Docker Desktop and achieve data persistence by mounting the volume postgres_data to /var/lib.postgresQL (instead of the old /data path). And set the environment variable POSTGRES_PASSWORD=secret. The container starts in backend mode without explicitly specifying port mapping or resource limits. I entered the container through docker exec to create a table and insert data. Then I deleted the container and restarted the new one with the same volume, verifying that the data still existed. 
During the operation, I noticed that PostgreSQL 18+ has new requirements for the volume mount path. An incorrect path will cause the container to fail to start. Finally, the problem was solved by checking the logs and adjusting the mount point, and the storage structure of the data file was confirmed in the Volumes interface of Docker Desktop.

3.4 Sharing Local Files with a Container
I created a local directory named public_html on Windows using Docker Desktop and wrote the index.html file in it.
Through PowerShell after entering the directory, run the command docker run - d - name my_site -p 8080:80 - v ${PWD} : / usr/local/apache2 / htdocs/HTTPD: 2.4 start the container, Mount the local directory to the root path of the container's website.
Accessing http://localhost:8080 successfully displayed the custom page, verifying that file synchronization was in effect.
I noticed that the slash at the end of the mount path and the current working directory must be accurate; otherwise, the default page will be loaded. In the experiment, no resource restrictions were set. By default, a bridged network was used, and port 8080 was mapped to port 80 of the container. This operation enabled me to master the basic usage and common configuration points of bind mount.

3.5 Multi-Container Application with Docker Compose
I cloned the nginx-node-redis project on Windows using Docker Desktop and started the application by executing docker compose up -d --build in the root directory of the project. Compose automatically creates the default network, connecting Nginx (mapping port 80 of the host machine), two Node.js containers (web1 and web2), and the Redis container together. I browsed http://localhost, refreshed and saw requests being processed alternately by web1 and web2, and counts persisted through shared Redis. The entire process does not require manual network configuration or starting containers one by one. I have learned to use compose.yml to centrally manage the dependencies, ports, and startup sequences of multi-container services. Compared with docker run, Compose is more concise, repeatable and easy to clean up.

4. Conclusion
This lab provided a solid foundation in Docker fundamentals. By combining conceptual understanding from the official documentation with practical experimentation, I learned how containers encapsulate applications, how images serve as blueprints, and how Docker Compose simplifies managing multi-service environments. These skills are essential for modern software development, deployment, and DevOps workflows, and I now feel confident using Docker for local development and prototyping.